# Discussion: Large Language Models – Productivity, Ethics, and Oversight  
*Module: Machine Learning (July 2025 B)*  
*Sarah Thompson – October 2025*  
[Collaborative Discussion 2: Legal and Ethical Views on ANN Applications](https://www.my-course.co.uk/mod/forum/view.php?id=1218681)
---

## Introduction  
This discussion explored how large language models (LLMs) such as ChatGPT affect productivity, creativity, and ethics across academic and professional settings. My initial post considered both benefits and risks, focusing on issues like bias, toxicity, and accountability. The conversation that followed expanded on these points, exploring responsible deployment, human oversight, and the role of AI literacy in balancing efficiency with ethical use.

---

## Initial Post  
<details>
<summary><b>View Full Initial Post (22 September 2025)</b></summary>

It is well established that large language models (LLMs) such as ChatGPT provide significant benefits to writers across industries, improving productivity, quality, and creativity. In administrative and academic work, they are used to summarise documents, generate drafts, and refine writing, reducing the time spent on repetitive tasks (Khalifa and Albadawy, 2024; Xu, 2025). In creative contexts, they can offer ideas and alternative phrasings. Doshi et al. (2024) show that generative AI enhances individual creativity, particularly for those with less experience or ability, though sometimes at the cost of diversity of thought.

These findings highlight a core challenge: LLMs are trained on vast amounts of real-world data, and because the real world is biased, those biases filter into the models. As Bender et al. (2021) warn, such systems risk becoming “stochastic parrots,” reproducing harmful patterns from their training data. In addition to bias, researchers highlight toxicity. Studies show that LLMs can be prompted to generate radicalising content (Hutson, 2021; Weidinger et al., 2021).

More recent developments have introduced stronger guardrails. OpenAI now publishes system cards documenting red-teaming and safety measures, while Meta has released Llama Guard (2023–2024), a classifier designed to filter unsafe prompts and outputs. These initiatives demonstrate progress, though independent research continues to find persistent issues with bias and toxicity (Stanford CRFM, 2023). A recent case involving ChatGPT encouraging a teen’s suicide (The Guardian, 2025) shows the continuing need for oversight and accountability.

</details>

---

## Peer Discussion Highlights  

**Abdulrahman Alhashmi (24 Sept 2025)**  
Suggested a three-step approach for responsible AI use: link tool use to task importance, verify outputs through reliable sources, and keep logs for transparency (Meta, 2024; OpenAI, 2023).  

**Tasnika Goorhoo (26 Sept 2025)**  
Agreed on the benefits of LLMs but emphasised bias, toxicity, and the importance of human oversight in mitigating harm.  

**Jordan Speight (30 Sept 2025)**  
Expanded on Tasnika’s point by discussing *automation bias*—the tendency to over-trust AI—and suggested training and “red flag” protocols to strengthen critical thinking (Romeo and Conti, 2025).  

---

## My Replies  

<details>
<summary><b>Reply to Lauretta Oghenevurie (5 October 2025)</b></summary>

The point that AI writers bring huge efficiency gains but can be detrimental to originality and critical thinking really resonates. The tension is evident in the examples from real workplaces, with other studies showing productivity gains, especially in administrative and support roles, at the cost of deeper engagement (Liu et al., 2025).

AI literacy does indeed feel like the missing piece, helping users understand how these tools work, where they can fail, and how to use them responsibly (Ng et al., 2021). That awareness could help avoid the “homogenisation” risk Jordan highlighted.

The copyright debate is also fascinating. As Kretschmer, Margoni and Oruç (2024) point out, AI training still sits in a grey legal area, but frameworks like those suggested by Wang et al. (2024) could help make things fairer for creators.

Overall, there is consensus here that AI can supercharge productivity, but it must be grounded in transparency, education, and ethical safeguards (Bender et al., 2021).

</details>

<details>
<summary><b>Reply to Eslam Salaheldin Abdelnaser Abdelhafez (5 October 2025)</b></summary>

The view that AI should act as a collaborator rather than a replacement is an interesting one, but complex in practice. Collaboration requires clear boundaries, including when to use AI, how to disclose it, and how to maintain an author’s intent. Without such guidance, there is a risk that dependency could replace discernment, a pattern consistent with the automation bias observed in human-AI collaboration studies (Romeo and Conti, 2025).

The focus on AI literacy is also crucial. In addition to teaching responsible use, there should also be an emphasis on helping students decide when not to rely on AI tools (Long and Magerko, 2020; Ng et al., 2021). Such awareness could help ensure that AI is used to enhance, not diminish, creativity and integrity in writing.

</details>

---

## Summary Post  
**Posted:** October 2025  

This discussion looked at both the advantages and risks of large language models such as ChatGPT. In my initial post, I explored how these tools improve productivity and creativity across academic, administrative, and creative work (Khalifa and Albadawy, 2024; Xu, 2025), while also noting concerns about bias, toxicity and accountability (Bender et al., 2021; The Guardian, 2025).  

From the replies, three main ideas stood out. Abdulrahman proposed clearer task boundaries and verification practices (Meta, 2024; OpenAI, 2023). Tasnika highlighted the role of human oversight, and Jordan pointed out that oversight alone is not enough without user training to question AI outputs (Romeo and Conti, 2025).  

In later discussions, I agreed with Lauretta and Eslam that AI writers should act as collaborators rather than replacements. Achieving this balance depends on AI literacy—understanding how these systems work, where they can fail, and how to use them responsibly (Ng et al., 2021).  

Overall, the consensus was that while AI can greatly enhance writing productivity, it must be guided by human judgement, transparency, and ethical awareness.  

---

## Reflection  

This discussion helped me connect theoretical ideas about AI ethics to practical applications. It reinforced how essential AI literacy is in managing risks such as bias, over-reliance, and homogenisation. I also realised that effective oversight isn’t just about inserting humans into the loop—it requires critical thinking, self-awareness, and clear governance. Moving forward, I plan to apply these lessons in both academic work and professional contexts, focusing on designing transparent and responsible AI-supported processes.

---

## Reference List  

Bender, E.M., Gebru, T., McMillan-Major, A. and Shmitchell, S. (2021) ‘On the dangers of stochastic parrots: Can language models be too big?’, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT ’21). New York: ACM, pp. 610–623. Available at: https://doi.org/10.1145/3442188.3445922
 (Accessed: 22 September 2025).

Doshi, A.R., Rocklage, M.D., Riedl, M.O. and Goldberg, A.E. (2024) ‘Generative AI enhances individual creativity but reduces the collective diversity of novel content’, Science Advances, 10(33), pp. 1–13. Available at: https://doi.org/10.1126/sciadv.adn5290
 (Accessed: 22 September 2025).

Hutson, M. (2021) ‘Robo-writers: The rise and risks of language-generating AI’, Nature, 591(7848), pp. 22–25. Available at: https://doi.org/10.1038/d41586-021-00530-0
 (Accessed: 22 September 2025).

Khalifa, M. and Albadawy, E. (2024) ‘Using artificial intelligence in academic writing and research: an essential productivity tool’, Discover Artificial Intelligence, 4(1), pp. 1–11. Available at: https://doi.org/10.1016/j.cmpbup.2024.100145
 (Accessed: 22 September 2025).

Kretschmer, M., Margoni, T. and Oruç, P. (2024) ‘Copyright law and the lifecycle of machine learning models’, IIC — International Review of Intellectual Property and Competition Law, 55(2), pp. 110–138. doi: 10.1007/s40319-023-01419-3.

Liu, D., Wang, J., Yang, H. and Zhang, X. (2025) ‘Examining the double-edged sword effect of AI usage on work engagement’, Frontiers in Psychology, 16, 1594713. doi: 10.3389/fpsyg.2025.1594713.

Long, D. and Magerko, B. (2020) ‘What is AI literacy? Competencies and design considerations’, Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI ’20), pp. 1–16. doi: 10.1145/3313831.3376727.

Meta (2024) Llama Guard 2: Model card. Available at: https://www.llama.com/
 (Accessed: 23 September 2025).

Ng, D.T.K., Leung, J.K.L., Chu, S.K.W. and Qiao, M.S. (2021) ‘Conceptualizing AI literacy: An exploratory review’, Computers and Education: Artificial Intelligence, 2, 100041. doi: 10.1016/j.caeai.2021.100041.

OpenAI (2023) GPT-4 system card. Available at: https://openai.com/
 (Accessed: 23 September 2025).

Romeo, G. and Conti, D. (2025) ‘Exploring automation bias in human–AI collaboration: a review and implications for explainable AI’, AI & Society. Available at: https://doi.org/10.1007/s00146-025-02422-7
 (Accessed: 30 September 2025).

Stanford Center for Research on Foundation Models (CRFM) (2023) Holistic Evaluation of Language Models (HELM). Stanford University. Available at: https://crfm.stanford.edu/helm
 (Accessed: 22 September 2025).

The Guardian (2025) ‘Teen killed himself after “months of encouragement from ChatGPT”, lawsuit claims’, The Guardian, 27 August. Available at: https://www.theguardian.com/technology/2025/aug/27/chatgpt-scrutiny-family-teen-killed-himself-sue-open-ai
 (Accessed: 22 September 2025).

Wang, J.T., Deng, Z., Chiba-Okabe, H., Barak, B. and Su, W.J. (2024) ‘An economic solution to copyright challenges of generative AI’, arXiv preprint arXiv:2404.13964. Available at: https://doi.org/10.48550/arXiv.2404.13964
 (Accessed: 5 October 2025).

Weidinger, L., Uesato, J., Rauh, M., Griffin, C., Huang, P.S., Krueger, G., Mellor, J., Glaese, A., Balle, B., Kasirzadeh, A. et al. (2021) ‘Ethical and social risks of harm from language models’, arXiv preprint arXiv:2112.04359. Available at: https://arxiv.org/abs/2112.04359
 (Accessed: 22 September 2025).

Wang, J.T., Deng, Z., Chiba-Okabe, H., Barak, B. and Su, W.J. (2024) ‘An economic solution to copyright challenges of generative AI’, arXiv preprint arXiv:2404.13964. Available at: https://doi.org/10.48550/arXiv.2404.13964
 (Accessed: 5 October 2025).

Xu, Z. (2025) ‘Patterns and purposes: A cross-journal analysis of AI tool usage in academic writing’, arXiv preprint arXiv:2502.00632. Available at: https://arxiv.org/abs/2502.00632
 (Accessed: 22 September 2025).

---
